{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Addendum for BDT on HEP Dataset ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhEB0m+milynGaEKMqC/IH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zaidanebreak/Classification-Model-for-ATLAS-Higgs-Boson-Machine-Learning-Challenge-2014/blob/main/Addendum_for_BDT_on_HEP_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqGkkNGQmCA"
      },
      "source": [
        "# DER vs. PRI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsHtWMSRP-O"
      },
      "source": [
        "In this note book we will apply the two models on DER features alone and on PRI features alone with replacing the Extreme Value and we will compare the results with PRI + DER "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuRgSgBl0SVz"
      },
      "source": [
        "### Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn_YvRKool6F"
      },
      "source": [
        "import os    #provides functions for interacting with the operating system.\n",
        "import numpy as np #provides a large set of numeric datatypes that you can use to construct arrays\n",
        "import pandas as pd #work with dataframes \n",
        "import matplotlib.pyplot as plt #manipulate elements of a figure, such as creating a figure, creating a plotting area, plotting lines, adding plot labels, etc\n",
        "from IPython.display import display, HTML \n",
        "import time \n",
        "np.random.seed(10) #to make the np.random yields the same result when used again. "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IIczjWS0YWA"
      },
      "source": [
        "### Instal the two Models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ6uol9Eop6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e022bed-b232-44ec-b8e7-81f12a45a3c4"
      },
      "source": [
        "!pip install xgboost --upgrade\n",
        "pass\n",
        "import xgboost #XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n",
        "import lightgbm #LightGBM is a gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage and it is a fast processing algorithm.\n",
        "print(\"xgboost Version is: \" + xgboost.__version__)\n",
        "print(\"lightgbm Version is: \" + lightgbm.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "xgboost Version is: 1.4.2\n",
            "lightgbm Version is: 2.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cSTu71D0cbu"
      },
      "source": [
        "### Conect Your Google Drive with Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jI6nemCos38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cebbc18-b99a-4c8a-fa5d-3198ce79aad6"
      },
      "source": [
        "!pip install PyDrive\n",
        "import os \n",
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default() \n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Here a link will appear you press on it, it will direct you to a page where you select the google account and copy the key and paste it in the required box."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.32.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (57.2.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (21.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aajen9Fn0rdw"
      },
      "source": [
        "### Upload the Dataset From Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUi3yPpgoxon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30b9f11-0f8c-41eb-ebbf-0fa51941f375"
      },
      "source": [
        "download = drive.CreateFile({'id':'1hDw0S2idRsymTYDiDJkr0LYt-MOH_cFo'})  # here you juat take the ID part of the shared link file from google drive\n",
        "download.GetContentFile(\"atlas-higgs-challenge-2014-v2.csv\")  # Write the name of the file as in Google Drive\n",
        "\n",
        "datapath=''\n",
        "!ls -lrt\n",
        "filename=os.path.join(datapath, \"atlas-higgs-challenge-2014-v2.csv\")   # the name of the file \n",
        "dfall = pd.read_csv(filename)   # Let python read the dataset\n",
        "dfall = dfall.sample(frac=1).reset_index(drop=True) # to return a sample of the dataframe and reset the index if exists in the dataframe.\n",
        "from datetime import datetime \n",
        "print(\"now:\", datetime.now())\n",
        "print(\"File loaded with\", dfall.shape[0],\"events \")   # dfall.shape gives the dimension of the dataframe and with [0] gives the first  dim. which is the number of rows. "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 190972\n",
            "drwxr-xr-x 1 root root      4096 Jul 16 13:20 sample_data\n",
            "-rw-r--r-- 1 root root       720 Aug  5 09:37 adc.json\n",
            "-rw-r--r-- 1 root root 195543089 Aug  5 11:41 atlas-higgs-challenge-2014-v2.csv\n",
            "now: 2021-08-05 11:41:23.611670\n",
            "File loaded with 818238 events \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN0Jovk80wad"
      },
      "source": [
        "### Extract the Features which has Extreme Value "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3c9SVguS0ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c46afc2-6455-42e9-cb21-56f0f581e139"
      },
      "source": [
        "dfall_numeric = dfall.drop(columns = ['Label','KaggleSet'])\n",
        "Extreme_features = []\n",
        "print(\"The features which have the Extreme Value are:\")\n",
        "for i in dfall_numeric.columns:\n",
        "  if dfall_numeric[dfall_numeric <-950][i] in dfall_numeric[i].values:\n",
        "    print(i)\n",
        "    Extreme_features.append(i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features which have the Extreme Value are:\n",
            "DER_mass_MMC\n",
            "DER_deltaeta_jet_jet\n",
            "DER_mass_jet_jet\n",
            "DER_prodeta_jet_jet\n",
            "DER_lep_eta_centrality\n",
            "PRI_jet_leading_pt\n",
            "PRI_jet_leading_eta\n",
            "PRI_jet_leading_phi\n",
            "PRI_jet_subleading_pt\n",
            "PRI_jet_subleading_eta\n",
            "PRI_jet_subleading_phi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OtC-Duc1oDR"
      },
      "source": [
        "### Replace Extreme Value with the Mean Value for each Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChRUyctJS1FM"
      },
      "source": [
        "for i in Extreme_features:\n",
        "  dfall[i] = dfall[i].replace({-999.:dfall[dfall[i] > -900][i].mean()})  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkQ8LEjX1xUT"
      },
      "source": [
        "### Define smaller Dataframes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QlDBikWo6pM"
      },
      "source": [
        "target = dfall[\"Label\"]\n",
        "weights = dfall[\"Weight\"]\n",
        "dfall['Label_N'] = np.where(dfall['Label']=='s', 1,0)  \n",
        "Target = dfall['Label_N']\n",
        "DER_features = dfall[['DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
        "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
        "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
        "       'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
        "       'DER_lep_eta_centrality']]\n",
        "PRI_features = dfall[['PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi',\n",
        "       'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
        "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
        "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
        "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']]\n",
        "DER_and_PRI  = dfall[['DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
        "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
        "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
        "       'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
        "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi',\n",
        "       'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
        "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
        "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
        "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvc_LwiD7aSs"
      },
      "source": [
        "## DER Features:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLvXC0oV7oj_"
      },
      "source": [
        "### Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA7_1pmspY4w"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "train_size = 0.61106915102 # fraction of sample used for training\n",
        "\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "    train_test_split(DER_features, Target, weights, train_size=train_size, random_state = 31415)  # add random state instead of random seed\n",
        "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
        "y_train, y_test, weights_train, weights_test = \\\n",
        "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
        "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQF83AZK7v6z"
      },
      "source": [
        "### Rescaling the Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDcv3eqzpZ5z"
      },
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)  # applied the transformation calculated the line above\n",
        "\n",
        "\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "\n",
        "for i in range(len(class_weights_train)):\n",
        "    #training dataset: equalize number of background and signal\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] \n",
        "    #test dataset : increase test weight to compensate for sampling\n",
        "    weights_test[y_test == i] *= 1/(1-train_size) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTZsKlP73XI"
      },
      "source": [
        "### Define the AMS Function for Significance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZmYUd-ppd1T"
      },
      "source": [
        "from math import sqrt\n",
        "from math import log\n",
        "def amsasimov(s,b): # asimov significance arXiv:1007.1727 eq. 97  + footnote \n",
        "        if b<=0 or s<=0:\n",
        "            return 0\n",
        "        try:\n",
        "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
        "        except ValueError:\n",
        "            print(1+float(s)/b)\n",
        "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
        "        #return s/sqrt(s+b)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA218ZRS8DQn"
      },
      "source": [
        "### Applying XGB Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOQ5EbDwpgnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c13cd5-f386-4ba5-9fa3-3fe197c0d5b9"
      },
      "source": [
        "#np.random.seed(31415) # set the random seed  useful?  try checking without \n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features (not a bad idea to double check without it)\n",
        "# can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
        "# use_label_encoder and eval_metric to silence warning in 1.3.0\n",
        "xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',random_state = 31415)\n",
        "# HPO (==Hyper Parameter Optimization), check on the web https://xgboost.readthedocs.io/ for other parameters\n",
        "#xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,max_depth=10,n_estimators=100) \n",
        "\n",
        "\n",
        "\n",
        "starting_time = time.time()\n",
        "\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values) # note that XGB 1.3.X requires positive weight\n",
        "\n",
        "        \n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_xgb_DER = xgb.predict_proba(X_test)[:,1]\n",
        "y_pred_xgb_DER = y_pred_xgb_DER.ravel()\n",
        "y_pred_train_xgb_DER = xgb.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_xgb_DER = roc_auc_score(y_true=y_test, y_score=y_pred_xgb_DER,sample_weight=weights_test)\n",
        "print(\"auc test:\",auc_test_xgb_DER)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb_DER,))\n",
        "int_pred_test_sig_xgb_DER = [weights_test[(y_test ==1) & (y_pred_xgb_DER > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_xgb_DER = [weights_test[(y_test ==0) & (y_pred_xgb_DER > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_xgb_DER = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb_DER,int_pred_test_bkg_xgb_DER)]\n",
        "significance_xgb_DER = max(vamsasimov_xgb_DER)\n",
        "P = significance_xgb_DER\n",
        "print(\"Z:\",P)\n",
        "# To save model\n",
        "#xgb.save_model(\"XGBoost.model\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 6.633300304412842\n",
            "auc test: 0.9309371646230253\n",
            "auc train: 0.9044976996760066\n",
            "Z: 3.5423482572287464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXFYLfXY8JK1"
      },
      "source": [
        "### Learning Curve for Different Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4CvUql0q1-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "e227e0dd-933b-4a1e-9c64-5f6d835c32ac"
      },
      "source": [
        "#In principle could, use learning_curve in sklearn\n",
        "#However : it does not handle weights, it does not allow to control testing dataset size\n",
        "#from sklearn.model_selection import learning_curve\n",
        "#train_sizes,train_scores,test_scores=learning_curve(\n",
        "#     XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',n_estimators=10),\n",
        "#     X_train,y_train,\n",
        "#     train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1],                  \n",
        "#     scoring='roc_auc',cv=5)\n",
        "\n",
        "\n",
        "train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1]\n",
        "ntrains=[]\n",
        "test_aucs=[]\n",
        "train_aucs=[]\n",
        "times=[]\n",
        "z = []\n",
        "\n",
        "for train_size in train_sizes:\n",
        "  ntrain=int(len(X_train)*train_size)\n",
        "  print(\"training with \",ntrain,\" events\")\n",
        "  ntrains+=[ntrain]\n",
        "  starting_time = time.time()\n",
        "\n",
        "  # train using the first ntrain event of the training dataset\n",
        "  xgb.fit(X_train[:ntrain,], y_train[:ntrain], sample_weight=weights_train[:ntrain])\n",
        "  training_time = time.time( ) - starting_time\n",
        "  times+=[training_time]\n",
        "\n",
        "  # score on test dataset (always the same)\n",
        "  y_pred_xgb=xgb.predict_proba(X_test)[:,1]\n",
        "  auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test)\n",
        "  test_aucs+=[auc_test_xgb]\n",
        "\n",
        "  # score on the train dataset \n",
        "  y_train_xgb=xgb.predict_proba(X_train[:ntrain])[:,1]\n",
        "  auc_train_xgb = roc_auc_score(y_true=y_train[:ntrain], y_score=y_train_xgb,sample_weight=weights_train[:ntrain])\n",
        "  train_aucs+=[auc_train_xgb]\n",
        "  \n",
        "  int_pred_test_sig_xgb = [weights_test[(y_test ==1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "  int_pred_test_bkg_xgb = [weights_test[(y_test ==0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "  vamsasimov_xgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb,int_pred_test_bkg_xgb)]\n",
        "  significance_xgb = max(vamsasimov_xgb)\n",
        "  Z = significance_xgb\n",
        "  z+=[Z]\n",
        "\n",
        "dflearning=pd.DataFrame({\"Ntraining\":ntrains,\n",
        "                         \"test_auc\":test_aucs,\n",
        "                         \"train_auc\":train_aucs,\n",
        "                         \"time\":times,\n",
        "                         \"Significance\": z})\n",
        "display(dflearning)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training with  4999  events\n",
            "training with  24999  events\n",
            "training with  49999  events\n",
            "training with  99999  events\n",
            "training with  249999  events\n",
            "training with  374999  events\n",
            "training with  499999  events\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ntraining</th>\n",
              "      <th>test_auc</th>\n",
              "      <th>train_auc</th>\n",
              "      <th>time</th>\n",
              "      <th>Significance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4999</td>\n",
              "      <td>0.904117</td>\n",
              "      <td>0.998320</td>\n",
              "      <td>0.311163</td>\n",
              "      <td>2.720664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24999</td>\n",
              "      <td>0.918059</td>\n",
              "      <td>0.985584</td>\n",
              "      <td>0.705849</td>\n",
              "      <td>3.065408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49999</td>\n",
              "      <td>0.922556</td>\n",
              "      <td>0.972436</td>\n",
              "      <td>1.027768</td>\n",
              "      <td>3.263878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99999</td>\n",
              "      <td>0.925866</td>\n",
              "      <td>0.959812</td>\n",
              "      <td>1.698580</td>\n",
              "      <td>3.337204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>249999</td>\n",
              "      <td>0.929201</td>\n",
              "      <td>0.947322</td>\n",
              "      <td>3.617674</td>\n",
              "      <td>3.450497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>374999</td>\n",
              "      <td>0.930356</td>\n",
              "      <td>0.942979</td>\n",
              "      <td>5.313935</td>\n",
              "      <td>3.499675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>499999</td>\n",
              "      <td>0.930937</td>\n",
              "      <td>0.941380</td>\n",
              "      <td>6.649565</td>\n",
              "      <td>3.542348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ntraining  test_auc  train_auc      time  Significance\n",
              "0       4999  0.904117   0.998320  0.311163      2.720664\n",
              "1      24999  0.918059   0.985584  0.705849      3.065408\n",
              "2      49999  0.922556   0.972436  1.027768      3.263878\n",
              "3      99999  0.925866   0.959812  1.698580      3.337204\n",
              "4     249999  0.929201   0.947322  3.617674      3.450497\n",
              "5     374999  0.930356   0.942979  5.313935      3.499675\n",
              "6     499999  0.930937   0.941380  6.649565      3.542348"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SARhfZM-dWZ"
      },
      "source": [
        "### Applying LightGBM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTdAavnD-gBk",
        "outputId": "80fada1e-35d6-4285-82ef-1fea24e33919"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "#gbm = lgb.LGBMClassifier()\n",
        "gbm = lgb.LGBMClassifier(random_state = 31415)\n",
        "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web https://lightgbm.readthedocs.io/ for other parameters\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "\n",
        "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "#gbm.fit(X_train, y_train.values) #ma\n",
        "\n",
        "\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
        "y_pred_gbm = y_pred_gbm.ravel()\n",
        "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm, sample_weight = weights_test)\n",
        "print(\"auc test:\",auc_test_gbm)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
        "\n",
        "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
        "significance_gbm = max(vamsasimov_gbm)\n",
        "W = significance_gbm\n",
        "print(\"Z:\",W)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 5.922123193740845\n",
            "auc test: 0.9302855810413604\n",
            "auc train: 0.8955461342926818\n",
            "Z: 3.433129157908218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQbZIOGT6bI"
      },
      "source": [
        "## PRI Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIYIY2Hu8TaG"
      },
      "source": [
        "### Split the Data from PRI Features Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvUdC_DBT9BI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "train_size = 0.61106915102 # fraction of sample used for training\n",
        "\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "    train_test_split(PRI_features, Target, weights, train_size=train_size, random_state = 31415)  # add random state instead of random seed\n",
        "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
        "y_train, y_test, weights_train, weights_test = \\\n",
        "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
        "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esYKGqrR8ay9"
      },
      "source": [
        "### Rescaling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG8Ojh6hUDzO"
      },
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)  # applied the transformation calculated the line above\n",
        "\n",
        "\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "\n",
        "for i in range(len(class_weights_train)):\n",
        "    #training dataset: equalize number of background and signal\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] \n",
        "    #test dataset : increase test weight to compensate for sampling\n",
        "    weights_test[y_test == i] *= 1/(1-train_size) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llBhfIYg8iJ0"
      },
      "source": [
        "### Applying XGB Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIoplszUUHFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07995a3-750d-47be-f75f-0f9db1474912"
      },
      "source": [
        "#np.random.seed(31415) # set the random seed  useful?  try checking without \n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features (not a bad idea to double check without it)\n",
        "# can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
        "# use_label_encoder and eval_metric to silence warning in 1.3.0\n",
        "xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',random_state = 31415)\n",
        "# HPO (==Hyper Parameter Optimization), check on the web https://xgboost.readthedocs.io/ for other parameters\n",
        "#xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,max_depth=10,n_estimators=100) \n",
        "\n",
        "\n",
        "\n",
        "starting_time = time.time()\n",
        "\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values) # note that XGB 1.3.X requires positive weight\n",
        "\n",
        "        \n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_xgb_PRI = xgb.predict_proba(X_test)[:,1]\n",
        "y_pred_xgb_PRI = y_pred_xgb_PRI.ravel()\n",
        "y_pred_train_xgb_PRI = xgb.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_xgb_PRI = roc_auc_score(y_true=y_test, y_score=y_pred_xgb_PRI,sample_weight=weights_test)\n",
        "print(\"auc test:\",auc_test_xgb_PRI)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb_PRI,))\n",
        "int_pred_test_sig_xgb_PRI = [weights_test[(y_test ==1) & (y_pred_xgb_PRI > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_xgb_PRI = [weights_test[(y_test ==0) & (y_pred_xgb_PRI > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_xgb_PRI = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb_PRI,int_pred_test_bkg_xgb_PRI)]\n",
        "significance_xgb_PRI = max(vamsasimov_xgb_PRI)\n",
        "P = significance_xgb_PRI\n",
        "print(\"Z:\",P)\n",
        "# To save model\n",
        "#xgb.save_model(\"XGBoost.model\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 7.791740655899048\n",
            "auc test: 0.9094278155716543\n",
            "auc train: 0.8582719876347862\n",
            "Z: 2.7158951516173735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnLNt6Fj9Amn"
      },
      "source": [
        "### Learning Curve for Different Sizes of the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHq3Fp07UKEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "9886fe83-0155-4532-fb3b-96087b305d81"
      },
      "source": [
        "#In principle could, use learning_curve in sklearn\n",
        "#However : it does not handle weights, it does not allow to control testing dataset size\n",
        "#from sklearn.model_selection import learning_curve\n",
        "#train_sizes,train_scores,test_scores=learning_curve(\n",
        "#     XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',n_estimators=10),\n",
        "#     X_train,y_train,\n",
        "#     train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1],                  \n",
        "#     scoring='roc_auc',cv=5)\n",
        "\n",
        "\n",
        "train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1]\n",
        "ntrains=[]\n",
        "test_aucs=[]\n",
        "train_aucs=[]\n",
        "times=[]\n",
        "z = []\n",
        "\n",
        "for train_size in train_sizes:\n",
        "  ntrain=int(len(X_train)*train_size)\n",
        "  print(\"training with \",ntrain,\" events\")\n",
        "  ntrains+=[ntrain]\n",
        "  starting_time = time.time()\n",
        "\n",
        "  # train using the first ntrain event of the training dataset\n",
        "  xgb.fit(X_train[:ntrain,], y_train[:ntrain], sample_weight=weights_train[:ntrain])\n",
        "  training_time = time.time( ) - starting_time\n",
        "  times+=[training_time]\n",
        "\n",
        "  # score on test dataset (always the same)\n",
        "  y_pred_xgb=xgb.predict_proba(X_test)[:,1]\n",
        "  auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test)\n",
        "  test_aucs+=[auc_test_xgb]\n",
        "\n",
        "  # score on the train dataset \n",
        "  y_train_xgb=xgb.predict_proba(X_train[:ntrain])[:,1]\n",
        "  auc_train_xgb = roc_auc_score(y_true=y_train[:ntrain], y_score=y_train_xgb,sample_weight=weights_train[:ntrain])\n",
        "  train_aucs+=[auc_train_xgb]\n",
        "  \n",
        "  int_pred_test_sig_xgb = [weights_test[(y_test ==1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "  int_pred_test_bkg_xgb = [weights_test[(y_test ==0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "  vamsasimov_xgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb,int_pred_test_bkg_xgb)]\n",
        "  significance_xgb = max(vamsasimov_xgb)\n",
        "  Z = significance_xgb\n",
        "  z+=[Z]\n",
        "\n",
        "dflearning2=pd.DataFrame({\"Ntraining\":ntrains,\n",
        "                         \"test_auc\":test_aucs,\n",
        "                         \"train_auc\":train_aucs,\n",
        "                         \"time\":times,\n",
        "                         \"Significance\": z})\n",
        "display(dflearning2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training with  4999  events\n",
            "training with  24999  events\n",
            "training with  49999  events\n",
            "training with  99999  events\n",
            "training with  249999  events\n",
            "training with  374999  events\n",
            "training with  499999  events\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ntraining</th>\n",
              "      <th>test_auc</th>\n",
              "      <th>train_auc</th>\n",
              "      <th>time</th>\n",
              "      <th>Significance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4999</td>\n",
              "      <td>0.856103</td>\n",
              "      <td>0.998551</td>\n",
              "      <td>0.424485</td>\n",
              "      <td>1.923934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24999</td>\n",
              "      <td>0.885644</td>\n",
              "      <td>0.981923</td>\n",
              "      <td>0.881531</td>\n",
              "      <td>2.285830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49999</td>\n",
              "      <td>0.891502</td>\n",
              "      <td>0.965474</td>\n",
              "      <td>1.285469</td>\n",
              "      <td>2.370186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99999</td>\n",
              "      <td>0.900982</td>\n",
              "      <td>0.950013</td>\n",
              "      <td>2.028423</td>\n",
              "      <td>2.551077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>249999</td>\n",
              "      <td>0.906788</td>\n",
              "      <td>0.932424</td>\n",
              "      <td>4.191371</td>\n",
              "      <td>2.672110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>374999</td>\n",
              "      <td>0.908927</td>\n",
              "      <td>0.926616</td>\n",
              "      <td>6.301348</td>\n",
              "      <td>2.704752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>499999</td>\n",
              "      <td>0.909428</td>\n",
              "      <td>0.924269</td>\n",
              "      <td>8.288465</td>\n",
              "      <td>2.715895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ntraining  test_auc  train_auc      time  Significance\n",
              "0       4999  0.856103   0.998551  0.424485      1.923934\n",
              "1      24999  0.885644   0.981923  0.881531      2.285830\n",
              "2      49999  0.891502   0.965474  1.285469      2.370186\n",
              "3      99999  0.900982   0.950013  2.028423      2.551077\n",
              "4     249999  0.906788   0.932424  4.191371      2.672110\n",
              "5     374999  0.908927   0.926616  6.301348      2.704752\n",
              "6     499999  0.909428   0.924269  8.288465      2.715895"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llQXlVOs-rV6"
      },
      "source": [
        "### Applying LightGBM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjBJLCoK-t3j",
        "outputId": "b30b3a91-b937-4909-9baa-5ff90f9a77c3"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "#gbm = lgb.LGBMClassifier()\n",
        "gbm = lgb.LGBMClassifier(random_state = 31415)\n",
        "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web https://lightgbm.readthedocs.io/ for other parameters\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "\n",
        "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "#gbm.fit(X_train, y_train.values) #ma\n",
        "\n",
        "\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
        "y_pred_gbm = y_pred_gbm.ravel()\n",
        "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm, sample_weight = weights_test)\n",
        "print(\"auc test:\",auc_test_gbm)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
        "\n",
        "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
        "significance_gbm = max(vamsasimov_gbm)\n",
        "W = significance_gbm\n",
        "print(\"Z:\",W)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 6.949141502380371\n",
            "auc test: 0.8843006186260567\n",
            "auc train: 0.8166981619963126\n",
            "Z: 2.310575991981444\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}